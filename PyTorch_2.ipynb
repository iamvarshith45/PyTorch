{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sQYaIIlTOPg2"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dvfp1BqdWmam",
        "outputId": "fc79310d-ab00-4626-f079-42bb01fcd8b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd is a core component of PyTorch that provides automatic differentiation for tensor operations. It enables gradient computation, which is essential for training machine learning models using optimization algorithms like gradient descent."
      ],
      "metadata": {
        "id": "tmf_-ywYfEum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dy_dx(x):\n",
        "\n",
        "  return (2 * x)\n",
        "\n",
        "\n",
        "dy_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4wFh_ZrWpbt",
        "outputId": "6df37e12-08a9-460a-e4c7-30114cc6b339"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, gradient means the rate of change of a function (like the loss function) with respect to change in its input parameters (weights and biases), and is used by algorithms like gradient descent, SGD, Adamoptimizer to iteratively adjust these parameters to minimize the loss and improve model accuracy."
      ],
      "metadata": {
        "id": "JSJuEGlnfZJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad = True) # pytorch undertands that it needs to compute gradient of x, initially it is default set to False\n",
        "y = x ** 2\n",
        "\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjze_HNTWxrz",
        "outputId": "ac599ab5-9689-41e4-bf08-084316dac05d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0apB0PuXByz",
        "outputId": "7dcd1630-dd04-47b1-9896-4907d39bd2eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward # now pytorch automatically calculates the derivative (dy_dx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "VOMSK8Qrdy1b",
        "outputId": "d99fc748-57bd-4af2-e91e-291d47f48263"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Tensor.backward of tensor(9., grad_fn=<PowBackward0>)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch._tensor.Tensor.backward</b><br/>def backward(gradient=None, retain_graph=None, create_graph=False, inputs=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/torch/_tensor.py</a>Computes the gradient of current tensor wrt graph leaves.\n",
              "\n",
              "The graph is differentiated using the chain rule. If the tensor is\n",
              "non-scalar (i.e. its data has more than one element) and requires\n",
              "gradient, the function additionally requires specifying a ``gradient``.\n",
              "It should be a tensor of matching type and shape, that represents\n",
              "the gradient of the differentiated function w.r.t. ``self``.\n",
              "\n",
              "This function accumulates gradients in the leaves - you might need to zero\n",
              "``.grad`` attributes or set them to ``None`` before calling it.\n",
              "See :ref:`Default gradient layouts&lt;default-grad-layouts&gt;`\n",
              "for details on the memory layout of accumulated gradients.\n",
              "\n",
              ".. note::\n",
              "\n",
              "    If you run any forward ops, create ``gradient``, and/or call ``backward``\n",
              "    in a user-specified CUDA stream context, see\n",
              "    :ref:`Stream semantics of backward passes&lt;bwd-cuda-stream-semantics&gt;`.\n",
              "\n",
              ".. note::\n",
              "\n",
              "    When ``inputs`` are provided and a given input is not a leaf,\n",
              "    the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n",
              "    It is an implementation detail on which the user should not rely.\n",
              "    See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
              "\n",
              "Args:\n",
              "    gradient (Tensor, optional): The gradient of the function\n",
              "        being differentiated w.r.t. ``self``.\n",
              "        This argument can be omitted if ``self`` is a scalar.\n",
              "    retain_graph (bool, optional): If ``False``, the graph used to compute\n",
              "        the grads will be freed. Note that in nearly all cases setting\n",
              "        this option to True is not needed and often can be worked around\n",
              "        in a much more efficient way. Defaults to the value of\n",
              "        ``create_graph``.\n",
              "    create_graph (bool, optional): If ``True``, graph of the derivative will\n",
              "        be constructed, allowing to compute higher order derivative\n",
              "        products. Defaults to ``False``.\n",
              "    inputs (sequence of Tensor, optional): Inputs w.r.t. which the gradient will be\n",
              "        accumulated into ``.grad``. All other tensors will be ignored. If not\n",
              "        provided, the gradient is accumulated into all the leaf Tensors that were\n",
              "        used to compute the :attr:`tensors`.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 570);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad # this method helps in eliminating the manual initialization of derivative (dy_dx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvbPtWGYeRxW",
        "outputId": "516473a1-9c46-4a1a-fe6a-9a162afed085"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects.\n",
        "\n",
        "\n",
        "In this DAG, leaves are the input tensors, roots are the output tensors.\n",
        "By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule..\n",
        "\n",
        "\n",
        "In a forward pass, autograd does two things simultaneously:\n",
        "1. run the requested operation to compute a resulting tensor\n",
        "2. maintain the operation's gradient function in the DAG.\n",
        "\n",
        "\n",
        "The backward pass kicks off when .(backward) is called on the DAG root. autograd then:\n",
        "1. computes the gradients from each .grad_fn\n",
        "2. accumulates them in the respective tensor's .grad attribute\n",
        "3. using the chain rule, goes all the way to the leaf tensors."
      ],
      "metadata": {
        "id": "Q8_DmEuNhFCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs\n",
        "x = torch.tensor(6.7)  # Input feature\n",
        "y = torch.tensor(0.0)  # True label (binary)\n",
        "\n",
        "w = torch.tensor(1.0)  # Weight\n",
        "b = torch.tensor(0.0)  # Bias"
      ],
      "metadata": {
        "id": "lfSElr85ekyK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss Function\n",
        "# Binary Loss function\n",
        "\n",
        "def binary_cross_entropy_loss(prediction,target):\n",
        "  epsilon = 1e-8\n",
        "  prediction = torch.clamp(prediction,epsilon,1-epsilon)\n",
        "  loss = -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "5XrhIMf0p1iW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## forward pass\n",
        "\n",
        "z = w * x + b  # weighted sum\n",
        "y_prediction = torch.sigmoid(z)  # predicted probability using sigmoid activation function\n",
        "\n",
        "# compute binary cross-entropy loss\n",
        "loss = binary_cross_entropy_loss(y_prediction, y)\n",
        "print('loss : ',loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYdq23AfrHKr",
        "outputId": "6dcdac69-200e-4376-da5f-9f4c4200a4a9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss :  tensor(6.7012)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_2hbr_AArb1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}